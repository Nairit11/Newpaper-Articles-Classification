{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "1TrpYNXTgKv1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing the 20NewsGroups dataset consisting of 11314 articles in the training dataset and 7532  articles in the test dataset accross 20 classes."
      ]
    },
    {
      "metadata": {
        "id": "0OgD2WhhlN5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3b58d3b5-1a97-434d-c565-1b3bffba1476"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "test = fetch_20newsgroups(subset='test', shuffle=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qHqTygNfiJX5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we Vectorize the articles in the Corpus.\n",
        "For this we use sci-kit learn's CountVectorizer to create a sparse matrix of the count of each word in an article\n",
        "For better results we then calculate the inverse term frequency for the words using sci-kit learn's TfidfTransformer"
      ]
    },
    {
      "metadata": {
        "id": "E9Nj5GYYQwos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "e6114fb6-4563-4dd9-e46f-f8d1f8ead091"
      },
      "cell_type": "code",
      "source": [
        "print(train.target[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iwG-KEgXlPMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "outputId": "f8a0683c-5c83-4abf-89ff-b001730b0817"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(train.data)\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "print (X_train_tfidf[0:4,0:20000])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 56979)\t0.0574701540748513\n",
            "  (0, 75358)\t0.3538350134970617\n",
            "  (0, 123162)\t0.25970902457356887\n",
            "  (0, 118280)\t0.21186807208281694\n",
            "  (0, 50527)\t0.05461428658858725\n",
            "  (0, 124031)\t0.10798795154169123\n",
            "  (0, 85354)\t0.03696978508816317\n",
            "  (0, 114688)\t0.06214070986309587\n",
            "  (0, 111322)\t0.019156718024950434\n",
            "  (0, 123984)\t0.036854292634593756\n",
            "  (0, 37780)\t0.3813389125949312\n",
            "  (0, 68532)\t0.07325812342131598\n",
            "  (0, 114731)\t0.1444727551278406\n",
            "  (0, 87620)\t0.0356718631408158\n",
            "  (0, 95162)\t0.03447138409326312\n",
            "  (0, 64095)\t0.035420924271313554\n",
            "  (0, 98949)\t0.16068606055394935\n",
            "  (0, 90379)\t0.01992885995664587\n",
            "  (0, 118983)\t0.03708597805061915\n",
            "  (0, 89362)\t0.06521174306303765\n",
            "  (0, 79666)\t0.10936401252414275\n",
            "  (0, 40998)\t0.07801368196918111\n",
            "  (0, 92081)\t0.09913274493911224\n",
            "  (0, 76032)\t0.01921946305222309\n",
            "  (0, 4605)\t0.06332603952480324\n",
            "  :\t:\n",
            "  (0, 37565)\t0.03431760442478462\n",
            "  (0, 113986)\t0.17691750674853085\n",
            "  (0, 83256)\t0.08844382496462175\n",
            "  (0, 86001)\t0.07000411445838192\n",
            "  (0, 51730)\t0.09714744057976724\n",
            "  (0, 109271)\t0.10844724822064675\n",
            "  (0, 128026)\t0.06062209588975889\n",
            "  (0, 96144)\t0.10826904490745742\n",
            "  (0, 78784)\t0.0633940918806495\n",
            "  (0, 63363)\t0.08342748387969037\n",
            "  (0, 90252)\t0.03188936879541757\n",
            "  (0, 123989)\t0.08207027465330355\n",
            "  (0, 67156)\t0.0731344392274018\n",
            "  (0, 128402)\t0.059222940832778424\n",
            "  (0, 62221)\t0.029215279924278678\n",
            "  (0, 57308)\t0.15587170091577043\n",
            "  (0, 76722)\t0.06908779999621749\n",
            "  (0, 94362)\t0.05545703139014723\n",
            "  (0, 78955)\t0.059898568880615996\n",
            "  (0, 114428)\t0.05511105154696677\n",
            "  (0, 66098)\t0.09785515708314482\n",
            "  (0, 35187)\t0.09353930598317126\n",
            "  (0, 35983)\t0.037704485636198756\n",
            "  (0, 128420)\t0.042784990792830935\n",
            "  (0, 86580)\t0.1315711871424099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NAOjSZD9iSHC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Having got the sparse matrix, we would now apply classification Algorithms on this vectorized word matrix to predic classes for data in test dataset.\n",
        "Starting with K-Nearest Neighbours\n"
      ]
    },
    {
      "metadata": {
        "id": "mSh6vcpgjJMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c4a9571-d921-4522-ea7c-b1ed509cdcc6"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import neighbors\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', neighbors.KNeighborsClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "np.mean(predicted == test.target)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6591874668082847"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "BaZLIzBLl_Zp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we apply Support Vector Machine algorithm\n"
      ]
    },
    {
      "metadata": {
        "id": "8SeVsWJQlhOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb632bc8-284f-4e6b-83be-34295c3f8a05"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "np.mean(predicted == test.target)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8516994158258099"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "NDhk52VRl0pD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we apply Naive Bayes"
      ]
    },
    {
      "metadata": {
        "id": "o3hXM3H9kGIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df2aedd8-0812-4824-c143-07bc5a06287d"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "np.mean(predicted == test.target)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7738980350504514"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "yqLnUNgbmBps",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All this while we used Bag-Of-Words technique to vectorize the dataset.\n",
        "Here we apply ngrams technique to create the sparse matrix. Let's have an example as how n-grams is differnt from Bag-of-Words and what it actually does."
      ]
    },
    {
      "metadata": {
        "id": "ZXp_i8uzoRu1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "8017322a-4ffc-4af4-ecf4-5cd4caea4a48"
      },
      "cell_type": "code",
      "source": [
        "ngram_vectorizer = CountVectorizer()\n",
        "counts = ngram_vectorizer.fit_transform(['Anagh Anagh is Chutiya', 'Anmol is smart'])\n",
        "print(\"Bag-of-Words\")\n",
        "print(ngram_vectorizer.get_feature_names())\n",
        "print(counts.toarray().astype(int))\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
        "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
        "print(\"Bi-grams\")\n",
        "print(ngram_vectorizer.get_feature_names())\n",
        "print(counts.toarray().astype(int))\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))\n",
        "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
        "print(\"Tri-grams\")\n",
        "print(ngram_vectorizer.get_feature_names())\n",
        "print(counts.toarray().astype(int))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag-of-Words\n",
            "['anagh', 'anmol', 'chutiya', 'is', 'smart']\n",
            "[[2 0 1 1 0]\n",
            " [0 1 0 1 1]]\n",
            "Bi-grams\n",
            "[' a', 'ag', 'an', 'gh', 'h ', 'l ', 'mo', 'na', 'nm', 'ol']\n",
            "[[1 1 1 1 1 0 0 1 0 0]\n",
            " [1 0 1 0 0 1 1 0 1 1]]\n",
            "Tri-grams\n",
            "[' an', 'agh', 'ana', 'anm', 'gh ', 'mol', 'nag', 'nmo', 'ol ']\n",
            "[[1 1 1 0 1 0 1 0 0]\n",
            " [1 0 0 1 0 1 0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LHgbmYdlrimI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What if we need to apply Bag-of-2grams, or in other words club two consecutive words in a document, then vectorize"
      ]
    },
    {
      "metadata": {
        "id": "lraA3HVTrvp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e6bb79f9-11a9-4ff3-e147-ccc7a30eee3d"
      },
      "cell_type": "code",
      "source": [
        "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "counts = ngram_vectorizer.fit_transform(['Today it is Allahabad', 'Tomorrow it will be Prayagraj'])\n",
        "print(\"Bag-of-2grams\")\n",
        "print(ngram_vectorizer.get_feature_names())\n",
        "print(counts.toarray().astype(int))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag-of-2grams\n",
            "['be prayagraj', 'is allahabad', 'it is', 'it will', 'today it', 'tomorrow it', 'will be']\n",
            "[[0 1 1 0 1 0 0]\n",
            " [1 0 0 1 0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2GhI2nohoSSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will apply n-grams on our dataset and will then apply SVM classification algorithm. We will compare the accuracies for uni-gram, bi-gram and tri-gram vectorization on character level."
      ]
    },
    {
      "metadata": {
        "id": "ONMMNbOumXRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9f4594f0-d074-41ab-fee8-ef77c480dc1a"
      },
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(1, 1))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For uni-grams : \",np.mean(predicted == test.target))\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For bi-grams : \",np.mean(predicted == test.target))\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For tri-grams : \",np.mean(predicted == test.target))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For uni-grams :  0.15493892724375996\n",
            "For bi-grams :  0.6437865108868827\n",
            "For tri-grams :  0.806558682952735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6PADWu_yqtZr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now applying Bag-of-1gram(Same as bag of words), Bag-of-2grams and Bag-of-3grams to our dataset which by the way is out actual intention."
      ]
    },
    {
      "metadata": {
        "id": "svJR_6Anqs2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a3fad99a-4c23-425b-8c95-703c5423643e"
      },
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(1, 1))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For Bag-of-1-grams : \",np.mean(predicted == test.target))\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(2, 2))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For Bag-of-2-grams : \",np.mean(predicted == test.target))\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(3, 3))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For Bag-of-3-grams : \",np.mean(predicted == test.target))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Bag-of-1-grams :  0.8499734466277217\n",
            "For Bag-of-2-grams :  0.8021773765268189\n",
            "For Bag-of-3-grams :  0.7169410515135423\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
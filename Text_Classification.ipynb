{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "1TrpYNXTgKv1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing the 20NewsGroups dataset consisting of 11314 articles in the training dataset and 7532  articles in the test dataset accross 20 classes."
      ]
    },
    {
      "metadata": {
        "id": "0OgD2WhhlN5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d34d2e3b-28f5-45e6-d018-4a2cc5e9c4b7"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "test = fetch_20newsgroups(subset='test', shuffle=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qHqTygNfiJX5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we Vectorize the articles in the Corpus.\n",
        "For this we use sci-kit learn's CountVectorizer to create a sparse matrix of the count of each word in an article\n",
        "For better results we then calculate the inverse term frequency for the words using sci-kit learn's TfidfTransformer"
      ]
    },
    {
      "metadata": {
        "id": "iwG-KEgXlPMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "bfff00e1-f3ef-45ef-c596-d7888b65a0aa"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(train.data)\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "print (X_train_tfidf[0:4,0:20000])\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 4605)\t0.06332603952480324\n",
            "  (0, 16574)\t0.14155752531572685\n",
            "  (0, 18299)\t0.138749083899155\n",
            "  (1, 7797)\t0.13724375024886204\n",
            "  (1, 2927)\t0.05212944077716299\n",
            "  (2, 12197)\t0.05168179280403425\n",
            "  (2, 15032)\t0.07834044496813063\n",
            "  (2, 6449)\t0.0681281384860916\n",
            "  (2, 5023)\t0.13698619641739623\n",
            "  (2, 5811)\t0.2878251559842457\n",
            "  (2, 6028)\t0.10554465088856506\n",
            "  (2, 3412)\t0.0622873125208309\n",
            "  (3, 4155)\t0.05353413616615429\n",
            "  (3, 18618)\t0.14195950717692907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NAOjSZD9iSHC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Having got the sparse matrix, we would now apply classification Algorithms on this vectorized word matrix to predic classes for data in test dataset.\n",
        "Starting with K-Nearest Neighbours\n"
      ]
    },
    {
      "metadata": {
        "id": "mSh6vcpgjJMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c4a9571-d921-4522-ea7c-b1ed509cdcc6"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import neighbors\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', neighbors.KNeighborsClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "np.mean(predicted == test.target)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6591874668082847"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "BaZLIzBLl_Zp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we apply Support Vector Machine algorithm\n"
      ]
    },
    {
      "metadata": {
        "id": "8SeVsWJQlhOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb632bc8-284f-4e6b-83be-34295c3f8a05"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "np.mean(predicted == test.target)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8516994158258099"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "NDhk52VRl0pD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we apply Naive Bayes"
      ]
    },
    {
      "metadata": {
        "id": "o3hXM3H9kGIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df2aedd8-0812-4824-c143-07bc5a06287d"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "np.mean(predicted == test.target)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7738980350504514"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "yqLnUNgbmBps",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All this while we used Bag-Of-Words technique to vectorize the dataset.\n",
        "Here we apply ngrams technique to create the sparse matrix. Let's have an example as how n-grams is differnt from Bag-of-Words and what it actually does."
      ]
    },
    {
      "metadata": {
        "id": "ZXp_i8uzoRu1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "d417e8cc-9f82-4958-ef03-df7654f1126a"
      },
      "cell_type": "code",
      "source": [
        "ngram_vectorizer = CountVectorizer()\n",
        "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
        "print(\"Bag-of-Words\")\n",
        "print(ngram_vectorizer.get_feature_names())\n",
        "print(counts.toarray().astype(int))\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
        "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
        "print(\"Bi-grams\")\n",
        "print(ngram_vectorizer.get_feature_names())\n",
        "print(counts.toarray().astype(int))\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))\n",
        "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
        "print(\"Tri-grams\")\n",
        "print(ngram_vectorizer.get_feature_names())\n",
        "print(counts.toarray().astype(int))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag-of-Words\n",
            "['anagh', 'anmol']\n",
            "[[1 0]\n",
            " [0 1]]\n",
            "Bi-grams\n",
            "[' a', 'ag', 'an', 'gh', 'h ', 'l ', 'mo', 'na', 'nm', 'ol']\n",
            "[[1 1 1 1 1 0 0 1 0 0]\n",
            " [1 0 1 0 0 1 1 0 1 1]]\n",
            "Tri-grams\n",
            "[' an', 'agh', 'ana', 'anm', 'gh ', 'mol', 'nag', 'nmo', 'ol ']\n",
            "[[1 1 1 0 1 0 1 0 0]\n",
            " [1 0 0 1 0 1 0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LHgbmYdlrimI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What if we need to apply Bag-of-2grams, or in other words club two consecutive words in a document, then vectorize"
      ]
    },
    {
      "metadata": {
        "id": "lraA3HVTrvp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e6bb79f9-11a9-4ff3-e147-ccc7a30eee3d"
      },
      "cell_type": "code",
      "source": [
        "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "counts = ngram_vectorizer.fit_transform(['Today it is Allahabad', 'Tomorrow it will be Prayagraj'])\n",
        "print(\"Bag-of-2grams\")\n",
        "print(ngram_vectorizer.get_feature_names())\n",
        "print(counts.toarray().astype(int))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag-of-2grams\n",
            "['be prayagraj', 'is allahabad', 'it is', 'it will', 'today it', 'tomorrow it', 'will be']\n",
            "[[0 1 1 0 1 0 0]\n",
            " [1 0 0 1 0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2GhI2nohoSSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will apply n-grams on our dataset and will then apply SVM classification algorithm. We will compare the accuracies for uni-gram, bi-gram and tri-gram vectorization on character level."
      ]
    },
    {
      "metadata": {
        "id": "ONMMNbOumXRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9f4594f0-d074-41ab-fee8-ef77c480dc1a"
      },
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(1, 1))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For uni-grams : \",np.mean(predicted == test.target))\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For bi-grams : \",np.mean(predicted == test.target))\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For tri-grams : \",np.mean(predicted == test.target))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For uni-grams :  0.15493892724375996\n",
            "For bi-grams :  0.6437865108868827\n",
            "For tri-grams :  0.806558682952735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6PADWu_yqtZr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now applying Bag-of-1gram(Same as bag of words), Bag-of-2grams and Bag-of-3grams to our dataset which by the way is out actual intention."
      ]
    },
    {
      "metadata": {
        "id": "svJR_6Anqs2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a3fad99a-4c23-425b-8c95-703c5423643e"
      },
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(1, 1))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For Bag-of-1-grams : \",np.mean(predicted == test.target))\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(2, 2))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For Bag-of-2-grams : \",np.mean(predicted == test.target))\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(3, 3))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier())])\n",
        "text_clf.fit(train.data, train.target)\n",
        "predicted = text_clf.predict(test.data)\n",
        "print(\"For Bag-of-3-grams : \",np.mean(predicted == test.target))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Bag-of-1-grams :  0.8499734466277217\n",
            "For Bag-of-2-grams :  0.8021773765268189\n",
            "For Bag-of-3-grams :  0.7169410515135423\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mkNIY0KvSESc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1079
        },
        "outputId": "22920b76-aecc-44b7-a4fd-4d6e1ea94f0a"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import reuters\n",
        "\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "\n",
        "print('# of Training Samples: {}'.format(len(x_train)))\n",
        "print('# of Test Samples: {}'.format(len(x_test)))\n",
        "\n",
        "num_classes = max(y_train) + 1\n",
        "print('# of Classes: {}'.format(num_classes))\n",
        "\n",
        "index_to_word = {}\n",
        "for key, value in word_index.items():\n",
        "    index_to_word[value] = key\n",
        "\n",
        "print(' '.join([index_to_word[x] for x in x_train[0]]))\n",
        "print(y_train[0])\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(x_train[0])\n",
        "print(len(x_train[0]))\n",
        "\n",
        "print(y_train[0])\n",
        "print(len(y_train[0]))\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.metrics_names)\n",
        "\n",
        "['loss', 'acc']\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n",
            "# of Training Samples: 8982\n",
            "# of Test Samples: 2246\n",
            "# of Classes: 46\n",
            "the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs\n",
            "3\n",
            "[0. 1. 0. ... 0. 0. 0.]\n",
            "10000\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "46\n",
            "['loss', 'acc']\n",
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/20\n",
            "8083/8083 [==============================] - 6s 701us/step - loss: 1.3081 - acc: 0.7159 - val_loss: 0.9678 - val_acc: 0.7976\n",
            "Epoch 2/20\n",
            "8083/8083 [==============================] - 3s 392us/step - loss: 0.5143 - acc: 0.8853 - val_loss: 0.8791 - val_acc: 0.8165\n",
            "Epoch 3/20\n",
            "8083/8083 [==============================] - 3s 383us/step - loss: 0.2850 - acc: 0.9364 - val_loss: 0.9064 - val_acc: 0.8087\n",
            "Epoch 4/20\n",
            "8083/8083 [==============================] - 3s 398us/step - loss: 0.2221 - acc: 0.9474 - val_loss: 0.9619 - val_acc: 0.8020\n",
            "Epoch 5/20\n",
            "8083/8083 [==============================] - 3s 381us/step - loss: 0.1866 - acc: 0.9560 - val_loss: 0.9765 - val_acc: 0.7942\n",
            "Epoch 6/20\n",
            "8083/8083 [==============================] - 3s 376us/step - loss: 0.1964 - acc: 0.9526 - val_loss: 0.9922 - val_acc: 0.8087\n",
            "Epoch 7/20\n",
            "8083/8083 [==============================] - 3s 378us/step - loss: 0.1865 - acc: 0.9563 - val_loss: 1.0663 - val_acc: 0.7920\n",
            "Epoch 8/20\n",
            "8083/8083 [==============================] - 3s 380us/step - loss: 0.1810 - acc: 0.9562 - val_loss: 1.0605 - val_acc: 0.8009\n",
            "Epoch 9/20\n",
            "8083/8083 [==============================] - 3s 381us/step - loss: 0.1763 - acc: 0.9558 - val_loss: 1.1000 - val_acc: 0.7942\n",
            "Epoch 10/20\n",
            "8083/8083 [==============================] - 3s 385us/step - loss: 0.1680 - acc: 0.9589 - val_loss: 1.1422 - val_acc: 0.7831\n",
            "Epoch 11/20\n",
            "8083/8083 [==============================] - 3s 390us/step - loss: 0.1871 - acc: 0.9568 - val_loss: 1.1235 - val_acc: 0.7976\n",
            "Epoch 12/20\n",
            "8083/8083 [==============================] - 3s 391us/step - loss: 0.1693 - acc: 0.9584 - val_loss: 1.1637 - val_acc: 0.7964\n",
            "Epoch 13/20\n",
            "8083/8083 [==============================] - 3s 384us/step - loss: 0.1737 - acc: 0.9576 - val_loss: 1.1219 - val_acc: 0.8031\n",
            "Epoch 14/20\n",
            "8083/8083 [==============================] - 3s 381us/step - loss: 0.1672 - acc: 0.9597 - val_loss: 1.1767 - val_acc: 0.8020\n",
            "Epoch 15/20\n",
            "8083/8083 [==============================] - 3s 382us/step - loss: 0.1781 - acc: 0.9599 - val_loss: 1.1344 - val_acc: 0.7987\n",
            "Epoch 16/20\n",
            "8083/8083 [==============================] - 3s 387us/step - loss: 0.1721 - acc: 0.9600 - val_loss: 1.1581 - val_acc: 0.7964\n",
            "Epoch 17/20\n",
            "8083/8083 [==============================] - 3s 384us/step - loss: 0.1497 - acc: 0.9628 - val_loss: 1.1909 - val_acc: 0.8009\n",
            "Epoch 18/20\n",
            "8083/8083 [==============================] - 3s 380us/step - loss: 0.1857 - acc: 0.9618 - val_loss: 1.1847 - val_acc: 0.8031\n",
            "Epoch 19/20\n",
            "8083/8083 [==============================] - 3s 383us/step - loss: 0.1612 - acc: 0.9620 - val_loss: 1.2053 - val_acc: 0.7987\n",
            "Epoch 20/20\n",
            "8083/8083 [==============================] - 3s 378us/step - loss: 0.1735 - acc: 0.9590 - val_loss: 1.2072 - val_acc: 0.7998\n",
            "2246/2246 [==============================] - 0s 109us/step\n",
            "Test loss: 1.1828186692452707\n",
            "Test accuracy: 0.798753339269813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CoYDYFMtLvr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1476
        },
        "outputId": "239cf873-3339-430b-bb35-6a335f85182f"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import sklearn.datasets as skds\n",
        "np.random.seed(1237)\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "test = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "\n",
        "label_index = train.target\n",
        "label_names = train.target_names\n",
        "labelled_files = train.filenames\n",
        " \n",
        "data_tags = [\"filename\",\"category\",\"news\"]\n",
        "data_list = []\n",
        " \n",
        "# Read and add data from file to a list\n",
        "i=0\n",
        "for f in labelled_files:\n",
        "    data_list.append((f,label_names[label_index[i]],train.data[i]))\n",
        "    i += 1\n",
        "    \n",
        "train_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
        "\n",
        "label_index = test.target\n",
        "label_names = test.target_names\n",
        "labelled_files = test.filenames\n",
        "data_list = []\n",
        " \n",
        "# Read and add data from file to a list\n",
        "i=0\n",
        "for f in labelled_files:\n",
        "    data_list.append((f,label_names[label_index[i]],test.data[i]))\n",
        "    i += 1\n",
        "    \n",
        "test_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
        "\n",
        "train_posts = train_data['news'][:]\n",
        "train_tags = train_data['category'][:]\n",
        "train_files_names = train_data['filename'][:]\n",
        " \n",
        "test_posts = test_data['news'][:]\n",
        "test_tags = test_data['category'][:]\n",
        "test_files_names = test_data['filename'][:]\n",
        "\n",
        "# 20 news groups\n",
        "num_labels = 20\n",
        "vocab_size = 15000\n",
        "batch_size = 100\n",
        " \n",
        "# define Tokenizer with Vocab Size\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_posts)\n",
        " \n",
        "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
        "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
        " \n",
        "\n",
        "\t\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(vocab_size,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=30,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 512)               7680512   \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 20)                10260     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 7,953,428\n",
            "Trainable params: 7,953,428\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10182 samples, validate on 1132 samples\n",
            "Epoch 1/30\n",
            "10182/10182 [==============================] - 4s 351us/step - loss: 1.0525 - acc: 0.7220 - val_loss: 0.3409 - val_acc: 0.9090\n",
            "Epoch 2/30\n",
            "10182/10182 [==============================] - 3s 281us/step - loss: 0.1248 - acc: 0.9731 - val_loss: 0.3514 - val_acc: 0.9108\n",
            "Epoch 3/30\n",
            "10182/10182 [==============================] - 3s 286us/step - loss: 0.0453 - acc: 0.9945 - val_loss: 0.3259 - val_acc: 0.9205\n",
            "Epoch 4/30\n",
            "10182/10182 [==============================] - 3s 280us/step - loss: 0.0346 - acc: 0.9961 - val_loss: 0.3605 - val_acc: 0.9108\n",
            "Epoch 5/30\n",
            "10182/10182 [==============================] - 3s 284us/step - loss: 0.0588 - acc: 0.9915 - val_loss: 0.3825 - val_acc: 0.9037\n",
            "Epoch 6/30\n",
            "10182/10182 [==============================] - 3s 275us/step - loss: 0.0409 - acc: 0.9927 - val_loss: 0.3759 - val_acc: 0.9072\n",
            "Epoch 7/30\n",
            "10182/10182 [==============================] - 3s 271us/step - loss: 0.0299 - acc: 0.9953 - val_loss: 0.5015 - val_acc: 0.8984\n",
            "Epoch 8/30\n",
            "10182/10182 [==============================] - 3s 266us/step - loss: 0.0497 - acc: 0.9932 - val_loss: 0.4758 - val_acc: 0.9037\n",
            "Epoch 9/30\n",
            "10182/10182 [==============================] - 3s 267us/step - loss: 0.0459 - acc: 0.9932 - val_loss: 0.5005 - val_acc: 0.8984\n",
            "Epoch 10/30\n",
            "10182/10182 [==============================] - 3s 270us/step - loss: 0.0435 - acc: 0.9938 - val_loss: 0.4527 - val_acc: 0.8966\n",
            "Epoch 11/30\n",
            "10182/10182 [==============================] - 3s 269us/step - loss: 0.0438 - acc: 0.9935 - val_loss: 0.5070 - val_acc: 0.8975\n",
            "Epoch 12/30\n",
            "10182/10182 [==============================] - 3s 264us/step - loss: 0.0346 - acc: 0.9947 - val_loss: 0.5282 - val_acc: 0.8958\n",
            "Epoch 13/30\n",
            "10182/10182 [==============================] - 3s 267us/step - loss: 0.0430 - acc: 0.9938 - val_loss: 0.5691 - val_acc: 0.8949\n",
            "Epoch 14/30\n",
            "10182/10182 [==============================] - 3s 267us/step - loss: 0.0421 - acc: 0.9958 - val_loss: 0.5517 - val_acc: 0.8975\n",
            "Epoch 15/30\n",
            "10182/10182 [==============================] - 3s 266us/step - loss: 0.0360 - acc: 0.9956 - val_loss: 0.4930 - val_acc: 0.9108\n",
            "Epoch 16/30\n",
            "10182/10182 [==============================] - 3s 268us/step - loss: 0.0278 - acc: 0.9970 - val_loss: 0.4858 - val_acc: 0.9099\n",
            "Epoch 17/30\n",
            "10182/10182 [==============================] - 3s 265us/step - loss: 0.0218 - acc: 0.9978 - val_loss: 0.4952 - val_acc: 0.9143\n",
            "Epoch 18/30\n",
            "10182/10182 [==============================] - 3s 255us/step - loss: 0.0239 - acc: 0.9979 - val_loss: 0.4824 - val_acc: 0.9187\n",
            "Epoch 19/30\n",
            "10182/10182 [==============================] - 3s 259us/step - loss: 0.0232 - acc: 0.9978 - val_loss: 0.5440 - val_acc: 0.9134\n",
            "Epoch 20/30\n",
            "10182/10182 [==============================] - 3s 256us/step - loss: 0.0273 - acc: 0.9974 - val_loss: 0.5594 - val_acc: 0.9072\n",
            "Epoch 21/30\n",
            "10182/10182 [==============================] - 3s 261us/step - loss: 0.0325 - acc: 0.9961 - val_loss: 0.5542 - val_acc: 0.9072\n",
            "Epoch 22/30\n",
            "10182/10182 [==============================] - 3s 261us/step - loss: 0.0298 - acc: 0.9958 - val_loss: 0.5469 - val_acc: 0.9117\n",
            "Epoch 23/30\n",
            "10182/10182 [==============================] - 3s 257us/step - loss: 0.0367 - acc: 0.9953 - val_loss: 0.6046 - val_acc: 0.9011\n",
            "Epoch 24/30\n",
            "10182/10182 [==============================] - 3s 256us/step - loss: 0.0329 - acc: 0.9962 - val_loss: 0.5238 - val_acc: 0.9134\n",
            "Epoch 25/30\n",
            "10182/10182 [==============================] - 3s 265us/step - loss: 0.0317 - acc: 0.9968 - val_loss: 0.5544 - val_acc: 0.9099\n",
            "Epoch 26/30\n",
            "10182/10182 [==============================] - 3s 277us/step - loss: 0.0299 - acc: 0.9972 - val_loss: 0.5330 - val_acc: 0.9134\n",
            "Epoch 27/30\n",
            "10182/10182 [==============================] - 3s 298us/step - loss: 0.0259 - acc: 0.9973 - val_loss: 0.5573 - val_acc: 0.9117\n",
            "Epoch 28/30\n",
            "10182/10182 [==============================] - 3s 299us/step - loss: 0.0396 - acc: 0.9952 - val_loss: 0.7197 - val_acc: 0.8984\n",
            "Epoch 29/30\n",
            "10182/10182 [==============================] - 3s 300us/step - loss: 0.0346 - acc: 0.9954 - val_loss: 0.8791 - val_acc: 0.8807\n",
            "Epoch 30/30\n",
            "10182/10182 [==============================] - 3s 300us/step - loss: 0.0485 - acc: 0.9925 - val_loss: 0.8282 - val_acc: 0.8852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QguUhX17Pdm0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9099baa7-110c-4dc4-f756-57a3944ee208"
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        " \n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7532/7532 [==============================] - 1s 119us/step\n",
            "Test accuracy: 0.7796070094255456\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}